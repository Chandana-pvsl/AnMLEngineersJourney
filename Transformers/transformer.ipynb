{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fd194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4a5f9",
   "metadata": {},
   "source": [
    "# Reproducibilty\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb42cdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility across different libraries.\"\"\"\n",
    "\n",
    "    # PYTHONHASHSEED - It is an environment variable in Python that sets the seed for hash randomization. Hash randomization affects the ordering of elements in things like dictionaries, sets, and anything that relies on hash tables internally. PYTHONHASHSEED doesn't directly affect PyTorch computations like random tensors or model training. But it can indirectly affect training/evaluation in cases where you shuffle datasets that are organized as a dictionary or set or  split datasets based on a random ordering of keys.\n",
    "\n",
    "    # Since Python 3.7 dict preserves the insertion order i.e. the order of keys you insert into a dict is the order you will see them when you iterate over the dict later. While dict iteration order is now stable (insertion-based), the hash values themselves (i.e., what hash(key) returns) are still randomized if PYTHONHASHSEED isn't set. If any of the pytorch internal sampling, tokenization or workers shuffling depend on this hash, it will result in non-deterministic behaviour. \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multiple GPUs.\n",
    "    \n",
    "\n",
    "    # CuDNN is CUDA Deep Neural Network library made by NVIDIA. Itâ€™s a low-level GPU-accelerated library that PyTorch, TensorFlow, and other frameworks use under the hood to run things like convolutions, activation functions, RNNs). CuDNN has multiple implementations for convolutions e.g. GEMM-based, Winograd etc. Each of these are better for different scenarios such as some are faster for smaller kernels while others use less memory. Whenever CuDNN convolutions are called, a benchmarking is called to find the implementation that runs the fastest for the current set of parameters sizes. This implementation is then used in all the future calls. Whenever the parameters sizes changes the benchmark is rerun, thus introducing the non-determinism(difficult to compare the performance of different sizes). When disabled, the safest algorithm is chosen always which is deterministic across all runs but it comes at a cost of maybe diminished speed. Hence, if input sizes are fixed, use benchmarking to speed up but if input sizes keep changing disable benchmarking\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # The various CuDNN implementations themselves might be using RNGs(Random Number Generators). Adding torch.backends.cudnn.deterministic = True will ensure the algorithm behaves same every time. \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    print(f\"Random seed set as {seed_value}\")\n",
    "\n",
    "# Usage\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "# Finally, the main thing...how to identify the best seed. In theory, any random seed should not matter, models should on an average perform same across different seeds. But in practice what I have observed is that, results could slightly differ with different seeds especially when the model is unstable or datasets is small or even when the training is sensitive(e.g. RL training)\n",
    "# In practice some good practices are\n",
    "#   1. If you have resources go for a Seed Sweep i.e.run multiple times with different seeds to choose the one that gives better results else you can stick to using some commonly used seed values like 42, 0, 1234\n",
    "#   2. Also, one can assign different seed values to different libraries, but I find using the same seed for all initializations helps to reduce the confusion during comparing/monitoring\n",
    "#   3. Finally, always log your seed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebfba4",
   "metadata": {},
   "source": [
    "# Pseudo Code\n",
    "1. Vocab Layer input: B X text, output: B X N\n",
    "    Create Tokenizer for the data\n",
    "    Generate token to index mapping\n",
    "    Incase num_tokens< model sequence length we will [PAD] it\n",
    "2. Embedding Layer (Input: B X N,  Output: B X N X d_model)\n",
    "    For each token Id map it to the corresponding embedding\n",
    "    Create a Embedding Matrix V X d_model\n",
    "    The input and output layer can have the same weights which is called weghts tying\n",
    "3. Positional Encoding Layer - GPT uses learnable position embeddings (Input: B X N,  Output: B X N X d_model)\n",
    "    Create Positional Encoding for the Sequence Length S\n",
    "    for even position sin(pos/10000**(2*i/d_model))\n",
    "    for odd position cos(pos/10000**(2*i/d_model))\n",
    "4. Input Layer -> Embedding Layer + Positional Encoding Layer - (Input: B X N,  Output: B X N X d_model)\n",
    "5. Transformer Layer - (Input: B X N X d_model, Output: B X N X d_model)\n",
    "    1. Multi Head Attention\n",
    "        1. Single Head Causal Attentions (Input: B X N X d_model, Output: B X N X d_hidden)\n",
    "            Performs Softmax(Q @ K.T/sqroot(d_hidden)) @ V\n",
    "            Attention Scores will be masked\n",
    "            Parameters - W_q, W_k, W_v (d_model X d_hidden)\n",
    "        2. Stack Multiple Heads   Input: B X num_heads X N X d_hidden, Output: B X N X d_model\n",
    "            d_model = d_hidden * num_heads\n",
    "    2. LayerNorm - Normalization across feature dimension (Input: B X N X d_model, Ouput: B X N X d_model)\n",
    "        Parameters: scale (d_model) and shift (d_model)\n",
    "        Equation: scale*(x-mean)/std_dev + shift\n",
    "    3. Residual Connection: x + f(x) (Input: B X N X d_model, Ouput: B X N X d_model)\n",
    "    4. Feed Forward Network: (Input: B X N X d_model, Ouput: B X N X d_model)\n",
    "        Passes throught two linear layers\n",
    "        1. First Layer d_model X 4*d_model\n",
    "        2. Non-linearity - Relu, Gelu etc\n",
    "        4. Second Layer - 4*d_model X d_model\n",
    "    Transformer Layer Equations\n",
    "            x = x + Dropout(MHA(Layer_Norm1(x)))\n",
    "            x = x + Dropout(FeedForwardNetwork(Layer_Norm2(x)))\n",
    "6. Output Layer\n",
    "    1. Linear Layer (Input:B X N X d_model, Output: B X N X V)\n",
    "        Parameters - d_model X V\n",
    "    2. Softmax Layer (Input:B X N X V, Output: B X N X V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057277c3",
   "metadata": {},
   "source": [
    "# What I learnt today\n",
    "1. GPT doesn't use [PAD] tokens, the masking and fixed length are handled in attention masks\n",
    "2. The padding will only effect attention and no other layer because all other layers are run in parallel\n",
    "3. There is a concept of pre-norm vs post-norm. The later being legacy and the former being preferred\n",
    "4. Droput can be added to feed forward as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57489d",
   "metadata": {},
   "source": [
    "# Class Definitions\n",
    "\n",
    "We will create separate class definition for each layer so that we can plug in various variants to test which will be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9ffcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Tokenizer(ABC):\n",
    "    @abstractmethod\n",
    "    def create_index(self, text):\n",
    "        \"\"\"\n",
    "        This method create the index by applying methods like BPE, Wordpiece etc\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_index(self):\n",
    "        \"\"\"\n",
    "        This method returns the text token to idx mapping\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_reverse_index(self):\n",
    "        \"\"\"\n",
    "        This method returns the token idx to text mapping\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Based on the toneization technique, this method uses the index to generate the index.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "        Based on the toneization technique, this method uses the index to generate the index.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5fad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # self.embedding_weights = nn.Parameter()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This method takes in a batch of input tokens of size B X N and returns embeddings corresponding to them of size B X N X d_model\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1adf227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_weights = nn.Parameter()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This method takes in a batch of input lengths of each sequence in the batch of size B X 1 and returns positional embeddings corresponding to them of size B X N X d_model\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6658af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class performs the following equations\n",
    "    x = x + Dropout(MHA(Layer_Norm1(x)))\n",
    "    x = x + Dropout(FeedForwardNetwork(Layer_Norm2(x)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and returns output of size B X N X d_model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    This class performs the Layer Normalization. \n",
    "    x_norm = scale*(x-mean)/std_dev + shift where scale and shift are learable parameter of size (d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and performs normalization across the last dimension.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Sequential):\n",
    "    \"\"\"\n",
    "    This class applies a feed forward network over the input. It essentially passes the input through two linear layers\n",
    "        1. First Layer d_model X 4*d_model\n",
    "        2. Non-linearity - Relu, Gelu etc\n",
    "        4. Second Layer - 4*d_model X d_model\n",
    "    Takes input of size B X N X d_model and applies a feed forward network over it and returns an output of size B X N X d_model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, prob):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and applies a Dropout layer over it and returns an output of size B X N X d_model\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class applies the MultiHeadAttention Layer over the input. The Self-Attention mechanism is run in parallel each time with a different set of learned parameters for query, key and value projections. If the number of heads are represented num_heads, to keep the input and output dimensions constant we compute the the hidden size d_k = d_model/num_heads. All the outputs from the head are concatenated and passed through a Linear Layer and a dropout is applied on them.\n",
    "    MHA(x) = Linear(Concat([Self_Attention(x)_1, ...., Self_Attention(x)_num_heads]))\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and applies a MHA layer over it and returns an output of size B X N X d_model. Uses attention_mask to stop computing attention for padding tokens. \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N text token indices and attention maps and applies the Transformer over it and returns an output of size B X N X V\n",
    "        \"\"\"\n",
    "        pass \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10742bca",
   "metadata": {},
   "source": [
    "# Things to notes\n",
    "\n",
    "1. nn.Parameter - this is used when you need to define a tensor that is a learnable parameter if your pytorch model. These parameters are tracked by the nn.Module and can be accessed via model.parameters() for optimization.\n",
    "2. nn.Linear by default has the bias term to compute Wx + b which can disabled essentially making it just Wx. So in the absence of bias both nn.Linear and nn.parameter can be use for Wx.\n",
    "3. nn.Sequential vs nn.Module - All neural networks are implemented with nn.Module. If the layers are sequentially used (self.layer3(self.layer2(self.layer1(x)))), you can leverage nn.Sequential to not have to define the forward function of the model.\n",
    "4. torch.nn.functional (a.k.a. F) is a module in PyTorch that provides stateless, function-based implementations of various neural network operations (like activations, normalizations, etc.), as opposed to nn.Module which are stateful, class-based versions. In functional we pass all the pass all the parameters and its update is controlled outside\n",
    "E.g.    \n",
    "        \n",
    "        # nn.Module\n",
    "        self.linear = nn.Linear(128, 64)            \n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "        \n",
    "        #nn.Functional\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        def forward(self, x):\n",
    "            weight = self.weight  # Define yourself in __init__\n",
    "            bias = self.bias\n",
    "            return F.linear(x, weight, bias)\n",
    "5. Difference between embedding formulations - nn.Embedding vs nn.Linear vs nn.Parameter - we can use all these to define embedding matrices but their forward functions operate differently.\n",
    "    a. nn.Embedding selects the rows of the given matrix, given a list of integers\n",
    "    b. nn.Linear does the einsum operation ...d, d e -> ...e\n",
    "    c. nn.Parameter basically just makes a tensor trainable (receive gradients and updates on step). this is the lowest level you can go, so actually, you can define your entire deep neural network with just nn.Parameters and manually do all the above with gathers and matrix multiplies or einsums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c68bd",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b88a9a",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa419d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT uses BPE, we will implement and experiment with wordpiece and unigram later\n",
    "from collections import defaultdict\n",
    "\n",
    "class BytePairEncoding(Tokenizer):\n",
    "    def __init__(self, max_vocab_size):\n",
    "        \"\"\"\n",
    "        Init method for Byte Pair Encoding\n",
    "        :param vocab_size: maximum size of the vocabulary\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.id2token_index = None\n",
    "        self.token2id_index = None\n",
    "        self.pad_token = None\n",
    "        self.merge_rules = []\n",
    "\n",
    "    def _preprocess_text(self, texts):\n",
    "        tokenized_text = {}\n",
    "        initial_vocab = set()\n",
    "        for i, text in enumerate(texts):\n",
    "            tokenized_text[i] = list(text.strip())\n",
    "            initial_vocab.update(tokenized_text[i])\n",
    "        return tokenized_text, initial_vocab\n",
    "    \n",
    "    def _get_most_frequent_pair(self, tokenized_text):\n",
    "        token_freq_map = defaultdict(int)\n",
    "        for i, tokens in tokenized_text.items():\n",
    "            for token_1, token2 in zip(tokens, tokens[1:]):\n",
    "                token_freq_map[(token_1, token2)] += 1\n",
    "        if not token_freq_map:\n",
    "            return None, None\n",
    "        # max_frequency = max(token_freq_map.values())\n",
    "        # max_freq_pair = [i for i in token_freq_map if token_freq_map[i]==max_frequency][0]\n",
    "        max_freq_pair = max(token_freq_map, key=lambda x: token_freq_map[x])\n",
    "        return max_freq_pair, token_freq_map[max_freq_pair]\n",
    "    \n",
    "    def _merge_tokens(self, tokenized_text, max_freq_pair):\n",
    "        \"\"\"\n",
    "        This method merges the occurrence of max_freq_pair tokens in tokenized_text\n",
    "        :param tokenized_text: - A dictionary of index to list of tokens mapping\n",
    "        :param max_freq_pair: - a tuple of two tokens which have the max frequency of occurring\n",
    "        \"\"\"\n",
    "        for i, tokens in tokenized_text.items():\n",
    "            updated_tokens = []\n",
    "            j = 0\n",
    "            while j < len(tokens) - 1:\n",
    "                if tokens[j] == max_freq_pair[0] and tokens[j + 1] == max_freq_pair[1]:\n",
    "                    updated_tokens.append(tokens[j] + tokens[j + 1])\n",
    "                    j += 2\n",
    "                else:\n",
    "                    updated_tokens.append(tokens[j])\n",
    "                    j += 1\n",
    "            \n",
    "            if j == len(tokens) - 1:  # Handle the last token if it doesn't form a pair with max_freq_pair\n",
    "                updated_tokens.append(tokens[-1])\n",
    "            \n",
    "            tokenized_text[i] = updated_tokens\n",
    "        return tokenized_text\n",
    "\n",
    "\n",
    "    def create_index(self, texts):\n",
    "        \"\"\"\n",
    "        Step:\n",
    "        1. Start with an initial vocab\n",
    "        2. Find the frequency of pair of tokens in the input texts based the vocab\n",
    "        3. Add the most frequently occuring pair in the vocab\n",
    "        4. Repeat step 2-3 until vocab_size is reached\n",
    "        \n",
    "        :param text: The text based on which we are generating the vocab\n",
    "        :param initial_vocab: The initial vocabulary \n",
    "\n",
    "        \"\"\"\n",
    "        tokenized_text, vocab = self._preprocess_text(texts)\n",
    "        self.merge_rules = []\n",
    "        prev_vocab_size = len(vocab)\n",
    "        while len(vocab)<self.max_vocab_size:\n",
    "            max_freq_pair, max_frequency = self._get_most_frequent_pair(tokenized_text)\n",
    "            if not max_freq_pair:\n",
    "                break\n",
    "            self.merge_rules.append(max_freq_pair)\n",
    "            tokenized_text = self._merge_tokens(tokenized_text, max_freq_pair)\n",
    "            vocab.add(\"\".join(max_freq_pair))\n",
    "        \n",
    "        self.id2token_index = {i: token for i, token in enumerate(vocab)}\n",
    "        self.token2id_index = {token: i for i, token in enumerate(vocab)}\n",
    "        return self.token2id_index\n",
    "    \n",
    "    def get_index(self):\n",
    "        return self.token2id_index\n",
    "\n",
    "    def get_reverse_index(self):\n",
    "        return self.id2token_index\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Based on the toneization technique, this method uses the index to generate the index.\n",
    "        \"\"\"\n",
    "        tokens = {0: list(text.strip())}\n",
    "        for rule in self.merge_rules:\n",
    "            tokens = self._merge_tokens(tokens, rule)\n",
    "        return [self.token2id_index[token] for token in tokens[0]]\n",
    "         \n",
    "        \n",
    "    def decode(self, tokens):\n",
    "        return [self.id2token_index[token_idx] for token_idx in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "826e6129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding slowest [11, 0, 7, 8, 12]\n",
      "Decoding [19, 2, 10, 18, 1] ['t', 'er', 'igh', 'n', 'k']\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairEncoding(20)\n",
    "corpus = [\n",
    "    \"low\",\n",
    "    \"lower\",\n",
    "    \"lowest\",\n",
    "    \"newer\",\n",
    "    \"newest\",\n",
    "    \"wide\",\n",
    "    \"wider\",\n",
    "    \"widest\",\n",
    "    \"high\",\n",
    "    \"higher\",\n",
    "    \"highest\",\n",
    "    \"bright\",\n",
    "    \"brighter\",\n",
    "    \"brightest\",\n",
    "    \"dark\",\n",
    "    \"darker\",\n",
    "    \"darkest\",\n",
    "    \"slow\",\n",
    "    \"slower\"\n",
    "]\n",
    "index = bpe.create_index(corpus)\n",
    "print(\"Encoding slowest\", bpe.encode(\"slowest\"))\n",
    "print(\"Decoding [19, 2, 10, 18, 1]\", bpe.decode([19, 2, 10, 18, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b08338",
   "metadata": {},
   "source": [
    "## Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding_weights = nn.Embedding(vocab_size, d_model, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This method takes in a batch of input tokens of size B X N and returns embeddings corresponding to them of size B X N X d_model\n",
    "        :param x: Input tokens of shape B X N \n",
    "        \"\"\"\n",
    "        return self.embedding_weights(x)\n",
    "        \n",
    "# Ideally this is not necessary but we will keep it in case we want to plug in another type of initialization\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.pos_embeddings = nn.Embedding(self.seq_len, self.d_model, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pos_embeddings(x)\n",
    "    \n",
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class performs the following equations\n",
    "    x = x + Dropout(MHA(Layer_Norm1(x)))\n",
    "    x = x + Dropout(FeedForwardNetwork(Layer_Norm2(x)))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout_p_mha, dropout_p_ffn):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadAttention(num_heads, d_model)\n",
    "        self.layer_norm_1 = LayerNorm(d_model)\n",
    "        self.dropout_1 = Dropout(dropout_p_mha)\n",
    "        self.ffn  = FeedForwardNetwork(d_model)\n",
    "        self.layer_norm_2 = LayerNorm(d_model)\n",
    "        self.dropout_2 = Dropout(dropout_p_ffn)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and returns output of size B X N X d_model\n",
    "        \"\"\"\n",
    "        x, attention_mask = inputs[0], inputs[1]\n",
    "        x = x + self.dropout_1(self.MHA(self.layer_norm_1(x), attention_mask))\n",
    "        x = x + self.dropout_2(self.ffn(self.layer_norm_2(x)))\n",
    "        return [x, attention_mask]\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    This class performs the Layer Normalization. \n",
    "    x_norm = scale*(x-mean)/std_dev + shift where scale and shift are learable parameter of size (d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.scale = nn.Parameter(torch.ones(self.d_model, dtype=torch.float32))\n",
    "        self.shift = nn.Parameter(torch.zeros(self.d_model, dtype=torch.float32))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and performs normalization across the last dimension.\n",
    "        :param x: Tensor of shape B X N X d_model\n",
    "        \"\"\"\n",
    "        return (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps) * self.scale + self.shift\n",
    "\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class applies a feed forward network over the input. It essentially passes the input through two linear layers\n",
    "        1. First Layer d_model X 4*d_model\n",
    "        2. Non-linearity - Relu, Gelu etc\n",
    "        4. Second Layer - 4*d_model X d_model\n",
    "    Takes input of size B X N X d_model and applies a feed forward network over it and returns an output of size B X N X d_model\n",
    "\n",
    "    Few models started using 8/3*d_model as hidden_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, hidden_dim=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or 4*d_model\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model, dtype=torch.float32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*d_model, d_model, dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input and Output are of dimension B X N X d_model \n",
    "        \"\"\"\n",
    "        return self.ffn(x)\n",
    "\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, prob):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and applies a Dropout layer over it and returns an output of size B X N X d_model\n",
    "        \"\"\"\n",
    "        bernoulli_distribution = torch.distributions.Bernoulli(probs = 1-self.prob)\n",
    "        mask = ~bernoulli_distribution.sample(x.shape).bool()\n",
    "        # alternative\n",
    "        # mask = (torch.rand_like(x) < self.prob)  \n",
    "        x.masked_fill_(mask, 0)\n",
    "        x = x/(1-self.prob) # inverted dropout\n",
    "        return x\n",
    "\n",
    "def stable_softmax(x, dim=-1):\n",
    "    x = x - torch.max(x, dim=dim, keepdim=True).values\n",
    "    return torch.softmax(x, dim=dim)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class applies the MultiHeadAttention Layer over the input. The Self-Attention mechanism is run in parallel each time with a different set of learned parameters for query, key and value projections. If the number of heads are represented num_heads, to keep the input and output dimensions constant we compute the the hidden size d_k = d_model/num_heads. All the outputs from the head are concatenated and passed through a Linear Layer and a dropout is applied on them.\n",
    "    MHA(x) = Linear(Concat([Self_Attention(x)_1, ...., Self_Attention(x)_num_heads]))\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if self.d_model%num_heads:\n",
    "            raise Exception(\"d_model {d_model} is not divisible by num_heads {num_heads}\")\n",
    "        self.d_k = self.d_model/num_heads\n",
    "        self.W_Q = nn.Linear(self.d_model, self.d_model, bias=False, dtype=torch.float32)\n",
    "        self.W_K = nn.Linear(self.d_model, self.d_model, bias=False, dtype=torch.float32)\n",
    "        self.W_V = nn.Linear(self.d_model, self.d_model, bias=False, dtype=torch.float32)\n",
    "        self.out = nn.Linear(self.d_model, self.d_model, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and applies a MHA layer over it and returns an output of size B X N X d_model. Uses attention_mask to stop computing attention for padding tokens. Attention mask is of shape B X N\n",
    "        Steps:\n",
    "        1. Compute Query Key and Value Vectors\n",
    "        2. Split the Q, K, V tensors to inclue num_heads dimension B X N X d_model -> B X num_heads X N X d_k\n",
    "        3. Compute Attention scores\n",
    "        4. Create a mask combining both causal masks and attention mask\n",
    "        4. Compute attention weights using softmax\n",
    "        5. Multiply with values and reshape\n",
    "        6. Apply final linear layer and return\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        queries, keys, values = self.W_Q(x), self.W_K(x), self.W_V(x)\n",
    "        queries = queries.reshape(batch_size, seq_length, self.num_heads, -1).transpose(1, 2) # B X num_heads X N X d_k\n",
    "        keys = keys.reshape(batch_size, seq_length, self.num_heads, -1).transpose(1, 2) # B X num_heads X N X d_k\n",
    "        values = values.reshape(batch_size, seq_length, self.num_heads, -1).transpose(1, 2) # B X num_heads X N X d_k\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(2, 3) # B X num_heads X N X N\n",
    "        causal_mask = torch.triu(torch.ones(seq_length, seq_length, dtype=attention_scores.dtype), diagonal=1).bool()\n",
    "        attention_scores.masked_fill_(causal_mask, -torch.inf)\n",
    "\n",
    "        attention_mask = (1-attention_mask).bool().unsqueeze(1).unsqueeze(2)\n",
    "        attention_scores.masked_fill_(attention_mask, -torch.inf)\n",
    "        \n",
    "        attention_weights = stable_softmax(attention_scores/(self.d_k**0.5)) # B X num_heads X N X N\n",
    "        contexts = attention_weights @ values # B X num_heads X N X d_model\n",
    "        contexts = contexts.transpose(1, 2).reshape(batch_size, seq_length, -1) # B X N X d_model\n",
    "        contexts = self.out(contexts) # B X N X d_model\n",
    "        return contexts\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    x = x + Dropout(MHA(Layer_Norm1(x)))\n",
    "    x = x + Dropout(FeedForwardNetwork(Layer_Norm2(x)))\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, max_seq_length, num_layers, d_model, num_heads, dropout_p_ffn, dropout_p_mha):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.positional_encoding = PositionalEncoding(seq_len=max_seq_length, d_model=d_model)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerLayer(d_model, num_heads, dropout_p_ffn, dropout_p_mha) for _ in range(num_layers)])\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "        self.out_head = nn.Parameter(self.embedding.embedding_weights.weight)\n",
    "\n",
    "    def forward(self, input_ids, position_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N text token indices and attention maps and applies the Transformer over it and returns an output of size B X N X V\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids) + self.positional_encoding(position_ids) # B X N X d_model\n",
    "        x, _ = self.transformer_blocks([x, attention_mask]) # B X N X d_model\n",
    "        x = self.layer_norm(x) # B X N X d_model\n",
    "\n",
    "        # we are using weight tying here, to untie them initialise a new \n",
    "        # V X d_model\n",
    "        logits = x @ self.out_head.T # B X N X V\n",
    "        return logits\n",
    "    \n",
    "class TransformerLayerModified(nn.Module):\n",
    "    \"\"\"\n",
    "    This class performs the following equations\n",
    "    x = x + Dropout(MHA(Layer_Norm1(x)))\n",
    "    x = x + Dropout(FeedForwardNetwork(Layer_Norm2(x)))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout_p_mha, dropout_p_ffn):\n",
    "        super().__init__()\n",
    "        self.MHA = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout_p_mha)\n",
    "        self.ffn  = FeedForwardNetwork(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout_2 = nn.Dropout(dropout_p_ffn)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes input of size B X N X d_model and returns output of size B X N X d_model\n",
    "        \"\"\"\n",
    "        x, attention_mask = inputs[0], inputs[1]\n",
    "        x = x + self.dropout_1(self.MHA(self.layer_norm_1(x), attention_mask))\n",
    "        x = x + self.dropout_2(self.ffn(self.layer_norm_2(x)))\n",
    "        return [x, attention_mask]\n",
    "\n",
    "\n",
    "# class TransformerModified(nn.Module):\n",
    "#     \"\"\"\n",
    "#     x = x + Dropout(MHA(Layer_Norm1(x)))\n",
    "#     x = x + Dropout(FeedForwardNetwork(Layer_Norm2(x)))\n",
    "#     \"\"\"\n",
    "#     def __init__(self, vocab_size, max_seq_length, num_layers, d_model, num_heads, dropout_p_ffn, dropout_p_mha):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, d_model, dtype=torch.float32)\n",
    "#         self.positional_encoding = nn.Embedding(max_seq_length, d_model, dtype=torch.float32)\n",
    "#         self.transformer_blocks = nn.Sequential(*[TransformerLayer(d_model, num_heads, dropout_p_ffn, dropout_p_mha) for _ in range(num_layers)])\n",
    "#         self.layer_norm = nn.LayerNorm(d_model)\n",
    "#         self.out_head = nn.Parameter(self.embedding.weight)\n",
    "\n",
    "#     def forward(self, input_ids, position_ids, attention_mask):\n",
    "#         \"\"\"\n",
    "#         Takes input of size B X N text token indices and attention maps and applies the Transformer over it and returns an output of size B X N X V\n",
    "#         \"\"\"\n",
    "#         x = self.embedding(input_ids) + self.positional_encoding(position_ids) # B X N X d_model\n",
    "#         x, _ = self.transformer_blocks([x, attention_mask]) # B X N X d_model\n",
    "#         x = self.layer_norm(x) # B X N X d_model\n",
    "\n",
    "#         # we are using weight tying here, to untie them initialise a new \n",
    "#         # V X d_model\n",
    "#         logits = x @ self.out_head.T # B X N X V\n",
    "#         return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0405f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "max_seq_length = 8 \n",
    "num_layers = 2 \n",
    "d_model = 4 \n",
    "num_heads = 2 \n",
    "dropout_p_ffn = dropout_p_mha = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942717b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_emb = InputEmbedding(vocab_size, d_model)\n",
    "pos_enc = PositionalEncoding(max_seq_length, d_model)\n",
    "trfm_layer = TransformerLayer(d_model, num_heads, dropout_p_mha, dropout_p_ffn)\n",
    "layer_norm = LayerNorm(d_model)\n",
    "ffn = FeedForwardNetwork(d_model)\n",
    "dropout = Dropout(dropout_p_ffn)\n",
    "mha = MultiHeadAttention(num_heads, d_model)\n",
    "transformer = Transformer(vocab_size, max_seq_length, num_layers, d_model, num_heads, dropout_p_ffn, dropout_p_mha)\n",
    "transformer_modified = TransformerModified(vocab_size, max_seq_length, num_layers, d_model, num_heads, dropout_p_ffn, dropout_p_mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3009a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4])\n",
      "torch.Size([2, 8, 4])\n",
      "tensor([ 1.0608,  0.2083, -0.5778,  0.3255], grad_fn=<SliceBackward0>)\n",
      "After normalization\n",
      "tensor([ 1.2024, -0.0684, -1.2403,  0.1063], grad_fn=<SliceBackward0>)\n",
      "torch.Size([2, 8, 4])\n",
      "torch.Size([2, 8, 4])\n",
      "torch.Size([2, 8, 4])\n",
      "torch.Size([2, 8, 4])\n",
      "torch.Size([2, 8, 50])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "# max seq len, full attention mask\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), dtype=torch.long)\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long).repeat(batch_size, 1)\n",
    "attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long) \n",
    "\n",
    "emb = inp_emb(input_ids)\n",
    "print(emb.shape)\n",
    "pos_emb = pos_enc(position_ids)\n",
    "print(pos_emb.shape)\n",
    "\n",
    "print(emb[0][0][:])\n",
    "# Python standard dev computed sample variance i.e. /(n-1)\n",
    "emb_norm = layer_norm(emb)\n",
    "print(\"After normalization\")\n",
    "print(emb_norm[0][0][:])\n",
    "\n",
    "ffn_out = ffn(emb)\n",
    "print(ffn_out.shape)\n",
    "\n",
    "dropout_out = dropout(emb)\n",
    "print(dropout_out.shape)\n",
    "\n",
    "\n",
    "logits = mha(emb+pos_emb, attention_mask)\n",
    "print(logits.shape)\n",
    "\n",
    "layer_output = trfm_layer([emb, attention_mask])\n",
    "print(layer_output[0].shape)\n",
    "\n",
    "\n",
    "# final_output = transformer(input_ids, position_ids, attention_mask)\n",
    "# print(final_output.shape)\n",
    "\n",
    "final_output_2 = transformer_modified(input_ids, position_ids, attention_mask)\n",
    "print(final_output_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb55a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 50])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(vocab_size, max_seq_length, num_layers, d_model, num_heads, dropout_p_ffn, dropout_p_mha)\n",
    "final_output = transformer(input_ids, position_ids, attention_mask)\n",
    "print(final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f28119b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.7823e-01, -1.7284e-01,  8.8187e-01,  2.6132e+00, -8.7541e-02,\n",
       "           2.2772e+00,  2.6209e+00,  2.8109e-01, -2.6724e+00,  2.4952e+00,\n",
       "          -6.5319e-01, -6.2310e-01, -7.1868e-01, -1.0253e-01, -2.4708e+00,\n",
       "           7.0503e-01, -7.6724e-02, -1.1606e+00,  3.9110e+00, -1.0433e+00,\n",
       "          -2.6682e+00,  5.2357e-01, -3.6021e-01,  1.4262e+00, -2.0906e+00,\n",
       "          -1.9009e+00,  9.2040e-01,  5.3079e-01,  1.3948e+00, -3.7902e+00,\n",
       "           1.1983e+00, -9.6277e-01, -9.7645e-01,  1.7302e+00,  1.0513e+00,\n",
       "          -1.8984e+00,  1.7777e+00, -4.0452e-01,  3.5390e-01, -1.2254e+00,\n",
       "          -5.9447e-01, -2.5599e+00,  1.7808e+00, -4.1721e-01,  4.5951e-01,\n",
       "           2.4297e+00,  1.0210e+00,  2.4919e+00, -9.7310e-02, -1.1566e+00],\n",
       "         [ 3.5443e-01, -1.6540e+00,  1.1289e+00, -2.7177e+00,  1.1591e+00,\n",
       "          -9.0957e-01, -5.1747e-01,  1.2390e-01,  2.9665e+00, -1.7177e+00,\n",
       "           5.4432e-01,  1.6880e+00,  8.8386e-01,  3.7137e-01,  3.5914e+00,\n",
       "          -4.4106e-01, -1.7633e+00,  1.8785e+00, -3.2528e+00,  6.0533e-01,\n",
       "           3.5303e+00, -1.5563e+00,  4.9533e-01, -8.5042e-01,  1.6893e+00,\n",
       "           7.7765e-01, -1.3553e+00, -1.6628e+00, -3.0583e+00,  3.1431e+00,\n",
       "          -1.6084e+00,  3.3964e-01,  5.2820e-01, -1.3904e+00, -1.7537e+00,\n",
       "           1.3030e+00, -2.1269e+00, -3.2284e-01,  1.4334e+00,  1.4882e+00,\n",
       "           1.0799e-01,  2.3474e+00, -1.2929e+00, -1.0996e+00,  2.9817e-02,\n",
       "          -1.4753e+00, -9.1851e-01, -1.1877e+00,  7.7914e-01,  3.1240e+00],\n",
       "         [-4.7929e-02, -9.3146e-01,  6.0820e-01, -2.2939e+00,  5.4160e-01,\n",
       "          -1.6497e+00, -1.7801e+00,  2.7273e-01,  3.0728e+00, -1.7164e+00,\n",
       "           6.7961e-01,  1.2974e+00,  9.8779e-01,  5.8401e-01,  3.3492e+00,\n",
       "          -1.0816e+00, -1.1177e+00,  1.3538e+00, -4.2320e+00,  4.9970e-01,\n",
       "           3.1978e+00, -1.1423e+00, -2.3219e-01, -1.3508e+00,  1.8862e+00,\n",
       "           1.7455e+00, -1.2911e+00, -7.3687e-01, -3.8583e+00,  4.0250e+00,\n",
       "          -1.7610e+00,  2.4554e-01,  7.2013e-01, -1.7676e+00, -1.5439e+00,\n",
       "           1.3036e+00, -1.3418e+00, -8.3622e-02,  8.8510e-01,  1.3658e+00,\n",
       "           5.8121e-01,  2.3689e+00, -2.1651e+00,  5.7481e-02, -3.5637e-01,\n",
       "          -1.8729e+00, -8.2137e-01, -2.2790e+00,  6.9378e-01,  2.6855e+00],\n",
       "         [ 2.1901e+00,  2.5393e+00, -3.7181e+00, -1.2453e+00, -8.5705e-01,\n",
       "          -1.3979e+00, -1.6963e+00, -1.5681e+00, -9.5174e-01, -2.2225e+00,\n",
       "          -1.0362e-01, -1.5327e+00, -7.2557e-01, -1.3875e+00, -2.0392e+00,\n",
       "           1.3078e+00,  2.8146e+00, -1.4921e-01,  1.1455e+00,  1.5651e+00,\n",
       "          -1.0523e+00,  1.3787e+00,  1.9952e+00, -1.3451e-02,  5.6473e-01,\n",
       "          -3.0440e-03,  8.8941e-01,  1.3278e-03,  6.8852e+00, -8.7080e-01,\n",
       "           1.5007e+00,  2.0140e+00,  6.3497e-01,  1.9855e-01,  1.1238e+00,\n",
       "           1.6962e+00, -1.7014e+00,  1.1874e+00, -3.0040e+00, -2.2737e-01,\n",
       "          -1.8886e-01,  6.5578e-01,  1.3886e+00,  3.4060e-01, -8.8990e-02,\n",
       "          -1.4095e+00, -6.4730e-01, -1.1815e-01, -1.5280e+00, -3.7382e+00],\n",
       "         [-2.3804e+00, -1.1688e+00,  2.3795e+00,  1.3079e+00, -1.1223e-01,\n",
       "          -1.5472e-01, -6.8214e-01,  1.5371e+00,  1.3652e+00,  1.5801e+00,\n",
       "           3.7559e-01,  8.7466e-01,  8.8307e-01,  1.5250e+00,  1.7922e+00,\n",
       "          -2.1467e+00, -1.5313e+00, -4.4383e-01, -2.9456e+00, -1.3637e+00,\n",
       "           8.3249e-01, -6.9292e-01, -2.7155e+00, -9.0056e-01,  8.6969e-02,\n",
       "           1.6437e+00, -8.1757e-01,  1.2302e+00, -7.3349e+00,  2.5512e+00,\n",
       "          -1.6968e+00, -1.7504e+00, -1.4185e-01, -9.5528e-01, -8.3514e-01,\n",
       "          -1.2046e+00,  2.3363e+00, -6.3921e-01,  1.7939e+00,  2.0192e-01,\n",
       "           9.1880e-01, -1.7255e-01, -2.6996e+00,  1.3976e+00, -5.3413e-01,\n",
       "           3.1713e-01,  5.5449e-01, -1.7977e+00,  1.2258e+00,  2.8070e+00],\n",
       "         [-1.3415e+00,  1.9823e+00, -1.1663e+00,  2.1779e+00, -1.7623e+00,\n",
       "          -1.1842e+00, -2.5036e+00,  4.9407e-01, -7.7179e-01,  9.4193e-01,\n",
       "           1.1680e-01, -1.3557e+00, -7.3267e-03,  5.2375e-01, -1.6791e+00,\n",
       "          -1.4752e+00,  1.8118e+00, -1.9133e+00, -1.1587e+00, -6.7357e-01,\n",
       "          -1.9887e+00,  1.3788e+00, -2.1219e+00, -8.2773e-01, -2.6427e-01,\n",
       "           1.9373e+00,  5.5562e-01,  2.7757e+00, -1.5438e+00,  9.4132e-01,\n",
       "           7.4928e-02, -6.0168e-01,  1.6061e-01, -3.6303e-01,  1.0151e+00,\n",
       "          -7.1221e-01,  2.8432e+00,  5.2641e-01, -1.4370e+00, -8.2469e-01,\n",
       "           1.0753e+00, -9.3318e-01, -1.6938e+00,  3.0510e+00, -8.9209e-01,\n",
       "          -1.7492e-01,  6.5818e-01, -2.0502e+00, -3.0306e-01, -1.7388e+00],\n",
       "         [-5.7661e-02, -2.9879e+00,  2.9885e+00, -8.5000e-01,  1.9174e+00,\n",
       "           2.0383e+00,  3.3603e+00,  4.0860e-01,  7.4996e-01,  6.7563e-01,\n",
       "          -1.4138e-01,  1.8340e+00,  2.6746e-01,  2.3724e-01,  2.0975e+00,\n",
       "           6.9898e-01, -2.9674e+00,  1.5354e+00,  9.7334e-01, -3.6423e-01,\n",
       "           1.8363e+00, -1.7907e+00,  7.5004e-01,  9.1640e-01, -3.6035e-01,\n",
       "          -1.9204e+00, -7.8719e-01, -2.2854e+00, -1.9474e+00, -9.0910e-01,\n",
       "          -6.4556e-01, -6.3947e-01, -5.9656e-01,  4.5831e-01, -1.2770e+00,\n",
       "          -5.2046e-01, -1.3071e+00, -1.1031e+00,  2.7836e+00,  6.3929e-01,\n",
       "          -9.0544e-01,  9.5272e-02,  9.9936e-01, -2.8267e+00,  8.6983e-01,\n",
       "           1.2075e+00, -8.7336e-02,  2.1625e+00,  1.0120e+00,  3.1919e+00],\n",
       "         [ 1.7755e+00,  2.6356e+00, -3.7003e+00, -1.5312e+00, -1.0845e+00,\n",
       "          -2.3006e+00, -2.9818e+00, -1.3004e+00,  4.4695e-02, -2.5837e+00,\n",
       "           1.8836e-01, -1.3511e+00, -3.3349e-01, -9.9740e-01, -1.1458e+00,\n",
       "           4.8441e-01,  2.7895e+00, -1.2184e-01, -7.9548e-01,  1.5462e+00,\n",
       "          -3.1936e-01,  1.2659e+00,  1.3136e+00, -7.4217e-01,  1.2038e+00,\n",
       "           1.1627e+00,  5.1900e-01,  4.3515e-01,  4.8548e+00,  9.2874e-01,\n",
       "           8.1285e-01,  1.9061e+00,  9.3661e-01, -5.7450e-01,  7.6979e-01,\n",
       "           1.9711e+00, -1.4417e+00,  1.2636e+00, -2.9667e+00,  8.6879e-02,\n",
       "           3.1450e-01,  1.3046e+00,  9.3338e-02,  1.1408e+00, -4.5286e-01,\n",
       "          -2.1370e+00, -7.7635e-01, -1.5139e+00, -1.3058e+00, -3.0743e+00]],\n",
       "\n",
       "        [[ 7.8270e-01,  3.1707e+00, -3.4548e+00,  9.2628e-01, -1.7593e+00,\n",
       "          -1.2881e+00, -2.2355e+00, -8.8788e-01, -1.8492e+00, -6.2058e-01,\n",
       "          -1.6621e-01, -2.1802e+00, -7.5169e-01, -8.1629e-01, -3.2120e+00,\n",
       "           3.2455e-01,  3.3106e+00, -1.5378e+00,  1.1385e+00,  5.9878e-01,\n",
       "          -2.6573e+00,  2.0479e+00,  2.3463e-01, -1.7118e-01, -1.9969e-01,\n",
       "           7.1106e-01,  1.2581e+00,  1.7884e+00,  4.8912e+00, -1.0214e+00,\n",
       "           1.5228e+00,  1.0186e+00,  3.7273e-01,  3.4760e-01,  1.7509e+00,\n",
       "           4.7901e-01,  7.6487e-01,  1.1666e+00, -3.1718e+00, -9.6331e-01,\n",
       "           3.5378e-01, -6.3813e-01,  5.1095e-01,  2.0008e+00, -4.9723e-01,\n",
       "          -6.5693e-01,  1.1836e-01, -7.3631e-01, -1.4223e+00, -4.2931e+00],\n",
       "         [ 1.8056e+00, -1.0579e+00, -1.4261e-01, -3.1642e+00,  1.3116e+00,\n",
       "          -3.0726e-01,  5.5851e-01, -8.2264e-01,  1.6309e+00, -2.2903e+00,\n",
       "           1.7431e-01,  1.0900e+00,  1.8691e-01, -6.4298e-01,  2.0750e+00,\n",
       "           1.1359e+00, -9.0969e-01,  2.0429e+00, -5.7061e-01,  1.3054e+00,\n",
       "           2.6007e+00, -1.0938e+00,  2.2635e+00,  4.0376e-02,  1.2466e+00,\n",
       "          -7.3684e-01, -6.9158e-01, -2.4943e+00,  1.9397e+00,  7.4601e-01,\n",
       "          -3.0776e-01,  1.2960e+00,  4.2109e-01, -4.3238e-01, -1.0793e+00,\n",
       "           1.7399e+00, -3.4223e+00, -2.5416e-02,  4.4611e-01,  1.1705e+00,\n",
       "          -6.4808e-01,  2.0160e+00,  8.5333e-01, -2.2349e+00,  5.0617e-01,\n",
       "          -1.1986e+00, -1.1097e+00,  5.4439e-01,  1.2858e-03,  1.2215e+00],\n",
       "         [ 1.7583e-01, -1.3331e+00,  8.9101e-01, -2.5696e+00,  8.8400e-01,\n",
       "          -1.2827e+00, -1.1329e+00,  1.9244e-01,  3.0689e+00, -1.7552e+00,\n",
       "           6.1843e-01,  1.5287e+00,  9.4748e-01,  4.7537e-01,  3.5362e+00,\n",
       "          -7.5024e-01, -1.4802e+00,  1.6629e+00, -3.7754e+00,  5.7076e-01,\n",
       "           3.4340e+00, -1.3839e+00,  1.6399e-01, -1.1043e+00,  1.8158e+00,\n",
       "           1.2526e+00, -1.3471e+00, -1.2527e+00, -3.4753e+00,  3.6182e+00,\n",
       "          -1.7063e+00,  3.0698e-01,  6.3113e-01, -1.5949e+00, -1.6829e+00,\n",
       "           1.3322e+00, -1.7973e+00, -2.1146e-01,  1.1901e+00,  1.4567e+00,\n",
       "           3.3467e-01,  2.4027e+00, -1.7278e+00, -5.6812e-01, -1.5371e-01,\n",
       "          -1.6958e+00, -8.9107e-01, -1.7293e+00,  7.4839e-01,  2.9619e+00],\n",
       "         [ 4.8791e-01, -9.1818e-01,  3.0117e-01, -2.7763e+00,  7.2643e-01,\n",
       "          -1.5685e+00, -1.4969e+00, -3.4852e-02,  2.9826e+00, -2.1261e+00,\n",
       "           6.2217e-01,  1.2937e+00,  8.5937e-01,  2.8490e-01,  3.2679e+00,\n",
       "          -6.0529e-01, -1.0307e+00,  1.6334e+00, -3.7298e+00,  8.1356e-01,\n",
       "           3.3092e+00, -1.1685e+00,  4.2423e-01, -1.1587e+00,  1.9467e+00,\n",
       "           1.3383e+00, -1.2295e+00, -1.2121e+00, -2.5322e+00,  3.6055e+00,\n",
       "          -1.5155e+00,  6.1489e-01,  7.5216e-01, -1.6170e+00, -1.5253e+00,\n",
       "           1.6153e+00, -2.0366e+00, -1.6961e-02,  7.1305e-01,  1.4386e+00,\n",
       "           3.4305e-01,  2.5459e+00, -1.6027e+00, -4.4831e-01, -1.9580e-01,\n",
       "          -1.9676e+00, -9.9914e-01, -1.8500e+00,  5.2114e-01,  2.4100e+00],\n",
       "         [-2.2500e+00, -1.4984e+00,  2.5965e+00,  8.6181e-01,  1.4365e-01,\n",
       "          -1.5983e-01, -5.4722e-01,  1.5184e+00,  1.7403e+00,  1.3345e+00,\n",
       "           4.2512e-01,  1.1587e+00,  9.7966e-01,  1.5298e+00,  2.3038e+00,\n",
       "          -2.0884e+00, -1.8608e+00, -1.0240e-01, -3.2091e+00, -1.2541e+00,\n",
       "           1.3577e+00, -9.6421e-01, -2.5119e+00, -9.2975e-01,  2.8438e-01,\n",
       "           1.5832e+00, -1.0053e+00,  8.4843e-01, -7.5660e+00,  2.8183e+00,\n",
       "          -1.8777e+00, -1.6757e+00, -9.9432e-02, -1.0768e+00, -1.0998e+00,\n",
       "          -1.0184e+00,  1.9051e+00, -7.1588e-01,  2.0612e+00,  4.2168e-01,\n",
       "           8.5139e-01,  1.4395e-01, -2.7159e+00,  1.0543e+00, -4.6480e-01,\n",
       "           1.7738e-01,  4.1014e-01, -1.7721e+00,  1.3355e+00,  3.2784e+00],\n",
       "         [-2.3247e+00,  1.3822e-01,  1.1471e+00,  2.2058e+00, -9.2776e-01,\n",
       "          -4.2719e-01, -1.3781e+00,  1.3065e+00,  2.6906e-01,  1.7865e+00,\n",
       "           2.4013e-01, -1.2284e-01,  5.1721e-01,  1.2714e+00,  1.4582e-01,\n",
       "          -2.0825e+00, -1.7837e-01, -1.3392e+00, -2.1289e+00, -1.3564e+00,\n",
       "          -6.7277e-01,  2.4624e-01, -2.8906e+00, -8.5255e-01, -2.9113e-01,\n",
       "           1.8351e+00, -1.8920e-01,  2.2162e+00, -5.5526e+00,  1.7710e+00,\n",
       "          -9.8429e-01, -1.5752e+00, -1.2368e-01, -6.3492e-01,  3.2563e-02,\n",
       "          -1.3587e+00,  3.1304e+00, -2.2420e-01,  5.6491e-01, -3.8868e-01,\n",
       "           1.0722e+00, -8.3673e-01, -2.4461e+00,  2.3597e+00, -7.3876e-01,\n",
       "           3.9093e-01,  7.9911e-01, -1.9301e+00,  6.7428e-01,  9.4462e-01],\n",
       "         [ 1.4494e+00,  5.3281e-01, -1.0756e+00,  3.3665e-01,  2.1051e-01,\n",
       "           1.5331e+00,  2.2413e+00, -9.8749e-01, -2.4232e+00,  2.0137e-01,\n",
       "          -6.5345e-01, -8.4186e-01, -1.0405e+00, -1.2011e+00, -2.5199e+00,\n",
       "           2.0868e+00,  8.6409e-01, -1.5049e-01,  4.4571e+00,  4.4641e-01,\n",
       "          -1.9157e+00,  6.5284e-01,  1.9503e+00,  1.5429e+00, -1.2298e+00,\n",
       "          -2.4583e+00,  1.0529e+00, -8.5922e-01,  6.1926e+00, -4.0842e+00,\n",
       "           1.8887e+00,  7.6120e-01, -4.7321e-01,  1.7081e+00,  1.0933e+00,\n",
       "          -1.5197e-01, -9.4323e-01,  1.7134e-01, -9.3153e-01, -7.6007e-01,\n",
       "          -1.1059e+00, -1.2443e+00,  3.1187e+00, -1.5487e+00,  7.3533e-01,\n",
       "           1.1801e+00,  1.2023e-01,  2.9176e+00, -8.9989e-01, -2.4873e+00],\n",
       "         [-8.7403e-01, -2.6919e+00,  2.9222e+00, -1.3576e+00,  1.4012e+00,\n",
       "           2.1397e-01,  7.4032e-01,  9.2092e-01,  2.6535e+00, -3.6061e-02,\n",
       "           4.3061e-01,  2.1181e+00,  1.0202e+00,  9.9233e-01,  3.7517e+00,\n",
       "          -9.3437e-01, -2.9093e+00,  1.5235e+00, -2.8337e+00, -3.9697e-01,\n",
       "           3.1839e+00, -1.9423e+00, -6.1621e-01, -5.3332e-01,  8.8779e-01,\n",
       "           4.1838e-01, -1.4769e+00, -1.3476e+00, -5.8447e+00,  2.6195e+00,\n",
       "          -1.9603e+00, -8.3581e-01,  8.1986e-03, -1.0591e+00, -1.9156e+00,\n",
       "           2.1520e-02, -7.3578e-01, -9.1698e-01,  2.7585e+00,  1.2218e+00,\n",
       "           1.0912e-01,  1.3422e+00, -1.5609e+00, -1.1591e+00,  1.2884e-01,\n",
       "          -2.4212e-01, -3.2855e-01, -6.3097e-01,  1.4099e+00,  4.3676e+00]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5aa9b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0437e+00, -3.1565e+00, -2.1469e+00,  2.9604e-01, -1.4018e+00,\n",
       "          -7.8756e-01, -1.2427e+00,  1.0890e+00, -2.5194e+00, -8.3359e-01,\n",
       "          -3.0718e+00, -1.3658e+00, -5.1145e-01, -9.1401e-01, -3.6644e+00,\n",
       "          -1.3340e+00,  3.8116e+00,  8.2318e-01, -1.1969e+00,  4.1260e+00,\n",
       "          -1.5146e+00, -1.5938e+00, -1.1659e+00, -8.2088e-01, -7.6612e-01,\n",
       "          -1.6289e+00, -1.7641e+00,  2.9192e+00,  3.0643e+00, -1.5594e+00,\n",
       "           1.4681e+00,  2.7616e-01, -1.7640e+00, -8.8701e-01,  2.1285e+00,\n",
       "           2.9416e+00,  1.6224e+00, -1.5445e+00,  1.2851e+00, -2.7506e+00,\n",
       "          -4.8113e-01,  2.6820e-01, -8.0649e-01,  3.4474e+00,  2.6646e+00,\n",
       "          -4.5111e+00, -2.4852e-01,  5.2903e+00,  2.1930e-01, -1.8119e+00],\n",
       "         [ 3.8756e-01,  2.7280e+00,  8.7870e-01, -6.6129e-01,  6.9335e-01,\n",
       "          -1.0358e+00, -9.5327e-02, -1.7258e+00,  4.5726e+00,  1.9338e+00,\n",
       "          -6.0985e-01,  5.4086e-02,  3.9197e-01,  2.8689e+00,  2.0765e+00,\n",
       "           8.9890e-01, -2.0342e+00,  1.6585e+00,  7.7149e-01, -2.5608e+00,\n",
       "           3.6973e-01,  7.9502e-01, -4.9104e-01,  3.7478e-01, -4.8678e-01,\n",
       "           1.7495e+00, -1.6728e-01, -4.4006e+00, -6.2125e-01,  1.1414e+00,\n",
       "          -2.1031e+00, -3.8629e+00,  1.5629e+00,  4.5226e-01, -2.7859e-01,\n",
       "           5.0981e-01, -1.8714e+00,  4.5975e-01, -1.9875e+00,  1.5624e+00,\n",
       "           3.1396e-01, -9.4881e-01,  2.1508e+00, -3.3487e+00, -3.3111e+00,\n",
       "           4.8778e+00, -2.3088e+00, -2.2117e+00,  6.3816e-01, -4.8174e-01],\n",
       "         [-1.4487e+00,  3.2100e-02, -1.4850e+00, -3.6420e-01, -1.8185e-01,\n",
       "          -2.8390e+00, -1.0955e+00, -1.0331e+00,  2.6852e+00,  1.4235e+00,\n",
       "          -5.1152e+00, -2.1262e+00, -9.5849e-02,  1.9898e+00, -2.1223e+00,\n",
       "          -7.4330e-01,  2.3173e+00,  3.2830e+00, -7.4545e-01,  1.7597e+00,\n",
       "          -1.3764e+00, -1.1916e+00, -2.3145e+00, -2.3745e-01, -1.3310e+00,\n",
       "           2.3295e-01, -2.6568e+00, -1.9837e+00,  3.5402e+00, -3.5190e-01,\n",
       "          -1.0197e+00, -4.4284e+00, -3.5785e-01, -2.5129e-01,  2.8756e+00,\n",
       "           4.7056e+00, -3.6243e-01, -1.8575e+00, -5.9022e-01, -1.9496e+00,\n",
       "          -1.7351e-01, -1.1324e+00,  1.9525e+00,  1.2094e-01, -2.7464e-01,\n",
       "           5.8610e-01, -3.8509e+00,  3.9655e+00,  7.0925e-01, -3.2888e+00],\n",
       "         [-2.1611e+00,  7.6792e-01,  8.5872e-01,  1.0966e+00,  2.8686e+00,\n",
       "           6.9196e-03,  3.4144e+00,  7.3872e-01, -4.2432e+00, -1.9935e+00,\n",
       "           1.4423e+00, -8.4746e-01,  4.8897e-02, -5.2697e+00, -5.9154e-01,\n",
       "          -1.0093e+00,  2.4394e-01, -2.4357e+00, -9.8898e-01, -3.7249e-01,\n",
       "           9.1466e-01, -6.1597e-01,  7.4365e-01,  1.3683e+00,  2.2705e+00,\n",
       "          -9.3124e-01,  7.5986e-01,  3.6062e+00,  2.8661e-01,  2.5744e-01,\n",
       "           1.0289e+00,  5.7189e+00, -1.2780e+00,  1.1987e+00,  8.4958e-01,\n",
       "          -1.6606e+00,  1.2384e+00, -1.3605e+00,  3.0086e+00, -1.8564e+00,\n",
       "           6.9334e-02,  8.9687e-02, -1.4667e+00,  2.1144e+00,  4.7911e+00,\n",
       "          -3.0169e+00,  1.0836e+00, -6.2838e-01, -2.5399e+00,  5.6832e-01],\n",
       "         [ 1.8470e+00, -2.8700e+00, -1.5045e-01,  2.8377e-01, -1.9926e+00,\n",
       "           3.0025e+00, -9.0360e-01,  1.9148e+00, -3.8360e+00, -1.7286e+00,\n",
       "           3.5344e+00,  1.9526e+00, -3.0659e-01, -1.2624e+00,  3.9638e-03,\n",
       "           2.6836e-01, -2.2070e-03, -2.6114e+00,  3.7339e-01,  1.2010e+00,\n",
       "           1.9667e-01,  4.8326e-01,  1.7441e+00, -8.7958e-01,  2.1943e-01,\n",
       "          -1.2492e+00,  1.6963e+00,  3.5025e+00, -2.1855e+00, -8.9989e-01,\n",
       "           2.0907e+00,  3.7024e+00, -4.6769e-01, -8.4993e-01, -2.3012e+00,\n",
       "          -3.0305e+00,  1.2921e+00,  1.6616e+00,  6.3425e-01,  1.0014e+00,\n",
       "          -1.9058e-01,  1.6319e+00, -2.5867e+00,  1.8020e+00,  6.5391e-01,\n",
       "          -3.2160e+00,  4.3190e+00, -5.9226e-01,  2.4141e-01,  2.5411e+00],\n",
       "         [ 1.0198e+00, -2.9721e+00, -4.6323e-01,  4.5644e-01, -1.5642e+00,\n",
       "           2.3503e+00, -5.3976e-01,  1.9428e+00, -4.3554e+00, -1.9098e+00,\n",
       "           2.5116e+00,  1.2255e+00, -3.5942e-01, -1.9998e+00, -8.6327e-01,\n",
       "          -2.0286e-01,  8.4698e-01, -2.3586e+00, -8.2624e-02,  1.8357e+00,\n",
       "          -2.9183e-02, -2.2492e-02,  1.3186e+00, -7.1918e-01,  3.4177e-01,\n",
       "          -1.5273e+00,  1.1530e+00,  4.0710e+00, -1.1373e+00, -1.0512e+00,\n",
       "           2.2123e+00,  3.9731e+00, -9.4993e-01, -7.3245e-01, -1.3545e+00,\n",
       "          -2.1477e+00,  1.6053e+00,  8.7064e-01,  1.2323e+00, -1.0748e-02,\n",
       "          -2.5277e-01,  1.4383e+00, -2.5488e+00,  2.5468e+00,  1.7959e+00,\n",
       "          -4.0874e+00,  3.7217e+00,  5.4463e-01, -1.1032e-01,  1.8240e+00],\n",
       "         [-2.8585e+00, -2.2775e-01, -3.4163e-01,  1.0096e+00,  2.2328e+00,\n",
       "          -1.1885e+00,  2.4929e+00,  6.8819e-01, -3.8991e+00, -1.6701e+00,\n",
       "          -1.2905e+00, -1.9375e+00, -1.4256e-01, -4.6151e+00, -2.3979e+00,\n",
       "          -1.6169e+00,  2.1785e+00, -9.5748e-01, -1.5563e+00,  1.5023e+00,\n",
       "          -5.8239e-02, -1.4737e+00, -4.2878e-01,  9.6769e-01,  1.4738e+00,\n",
       "          -1.3053e+00, -7.1180e-01,  3.6525e+00,  2.4007e+00, -3.5181e-01,\n",
       "           1.0844e+00,  4.0270e+00, -1.8743e+00,  7.8181e-01,  2.4251e+00,\n",
       "           9.0573e-01,  1.5514e+00, -2.3891e+00,  3.0523e+00, -3.2607e+00,\n",
       "          -1.3995e-01, -2.1301e-01, -9.8392e-01,  3.1050e+00,  5.2735e+00,\n",
       "          -4.0505e+00, -3.4093e-01,  2.3572e+00, -2.1061e+00, -1.1206e+00],\n",
       "         [-2.2339e+00,  6.7424e-01,  7.6058e-01,  1.1005e+00,  2.8302e+00,\n",
       "          -8.4043e-02,  3.3611e+00,  7.5008e-01, -4.2703e+00, -1.9917e+00,\n",
       "           1.2321e+00, -9.4202e-01,  3.1106e-02, -5.2672e+00, -7.5572e-01,\n",
       "          -1.0709e+00,  4.1614e-01, -2.3407e+00, -1.0463e+00, -2.0567e-01,\n",
       "           8.3863e-01, -6.9481e-01,  6.5571e-01,  1.3417e+00,  2.2227e+00,\n",
       "          -9.7859e-01,  6.4556e-01,  3.6602e+00,  4.6481e-01,  2.0213e-01,\n",
       "           1.0531e+00,  5.6408e+00, -1.3445e+00,  1.1693e+00,  9.8518e-01,\n",
       "          -1.4648e+00,  1.2834e+00, -1.4558e+00,  3.0435e+00, -1.9924e+00,\n",
       "           5.0708e-02,  7.1533e-02, -1.4500e+00,  2.2296e+00,  4.8812e+00,\n",
       "          -3.1508e+00,  9.8865e-01, -3.7472e-01, -2.5244e+00,  4.3708e-01]],\n",
       "\n",
       "        [[-8.2415e-01, -2.3121e+00, -7.5247e-01,  7.5241e-01, -1.4625e-01,\n",
       "           7.1714e-01,  5.8535e-01,  1.5956e+00, -4.6210e+00, -1.9666e+00,\n",
       "           4.0959e-01, -3.5206e-01, -3.5020e-01, -3.3208e+00, -2.1210e+00,\n",
       "          -1.0334e+00,  2.0384e+00, -1.6560e+00, -9.1590e-01,  2.3637e+00,\n",
       "          -2.7857e-01, -9.0682e-01,  3.9367e-01, -1.2054e-01,  7.2368e-01,\n",
       "          -1.7095e+00,  8.0579e-02,  4.4108e+00,  8.7071e-01, -1.0057e+00,\n",
       "           1.9842e+00,  4.1059e+00, -1.6456e+00, -2.1992e-01,  6.0976e-01,\n",
       "          -3.8778e-01,  1.8614e+00, -7.8301e-01,  2.2501e+00, -1.8628e+00,\n",
       "          -2.8260e-01,  7.8103e-01, -2.0318e+00,  3.3712e+00,  3.7640e+00,\n",
       "          -4.8427e+00,  1.9661e+00,  2.1723e+00, -9.5182e-01,  2.8252e-01],\n",
       "         [-3.8331e+00,  2.7265e+00, -2.5830e-02,  4.5013e-01,  3.6064e+00,\n",
       "          -3.7444e+00,  2.7358e+00, -1.3917e+00,  9.6408e-01,  4.8690e-01,\n",
       "          -4.2513e+00, -3.2527e+00,  2.1141e-01, -2.1072e+00, -1.6187e+00,\n",
       "          -1.3897e+00,  1.4518e+00,  1.8010e+00, -1.4522e+00, -2.1699e-01,\n",
       "          -1.7823e-01, -1.4841e+00, -1.9589e+00,  1.5862e+00,  8.8893e-01,\n",
       "           3.1720e-01, -2.0972e+00, -8.3521e-01,  3.7544e+00,  6.7115e-01,\n",
       "          -1.2870e+00, -6.6059e-01, -8.4914e-01,  1.4244e+00,  3.9179e+00,\n",
       "           3.4878e+00, -1.7916e-01, -3.2935e+00,  1.5633e+00, -3.2459e+00,\n",
       "           9.9899e-02, -1.7448e+00,  1.8166e+00,  3.9318e-01,  3.1129e+00,\n",
       "           3.2524e-01, -4.4208e+00,  2.1000e+00, -1.7760e+00, -3.2112e+00],\n",
       "         [-2.4028e+00,  4.6001e+00,  2.4712e+00,  7.1570e-01,  4.9360e+00,\n",
       "          -1.0785e+00,  4.7629e+00, -1.1032e+00, -2.7911e-01, -4.2260e-01,\n",
       "           1.6862e+00, -9.3127e-01,  5.8364e-01, -3.8596e+00,  2.0412e+00,\n",
       "          -1.8846e-01, -2.4913e+00, -1.5628e+00, -3.2439e-01, -3.9874e+00,\n",
       "           1.8823e+00,  2.6861e-01,  6.1269e-01,  2.4246e+00,  2.6732e+00,\n",
       "           9.3426e-01,  1.0841e+00, -4.5392e-01, -7.1322e-01,  1.8674e+00,\n",
       "          -1.1848e+00,  3.4202e+00,  2.5888e-01,  2.2882e+00,  5.7797e-01,\n",
       "          -2.0856e+00, -6.4955e-01, -1.1503e+00,  1.7037e+00, -4.1272e-01,\n",
       "           5.1834e-01, -1.0006e+00,  5.5026e-01, -1.3727e+00,  2.4655e+00,\n",
       "           2.0206e+00, -1.1243e+00, -4.0898e+00, -2.8006e+00,  4.6472e-01],\n",
       "         [-4.2074e+00,  3.6030e+00,  9.0065e-01,  8.3769e-01,  4.9304e+00,\n",
       "          -3.1961e+00,  4.3522e+00, -1.1917e+00, -4.4942e-01, -2.6681e-01,\n",
       "          -2.4332e+00, -2.9994e+00,  3.4037e-01, -4.1004e+00, -8.3322e-01,\n",
       "          -1.3929e+00,  4.8622e-01,  3.1220e-01, -1.4741e+00, -1.3867e+00,\n",
       "           6.3199e-01, -1.2339e+00, -1.1466e+00,  2.2678e+00,  2.0361e+00,\n",
       "           2.6287e-01, -1.1167e+00,  1.7542e-01,  2.7774e+00,  1.1116e+00,\n",
       "          -1.0573e+00,  1.9553e+00, -9.1990e-01,  2.0598e+00,  3.4149e+00,\n",
       "           1.6732e+00,  4.6700e-03, -3.2275e+00,  2.4791e+00, -3.1138e+00,\n",
       "           2.4119e-01, -1.6066e+00,  1.1732e+00,  4.7434e-01,  4.3601e+00,\n",
       "           2.5794e-02, -3.4488e+00,  3.0846e-01, -2.8285e+00, -2.2024e+00],\n",
       "         [ 2.1805e+00, -3.3101e+00, -4.0120e-01,  1.6526e-01, -2.5732e+00,\n",
       "           3.1138e+00, -1.4925e+00,  1.9774e+00, -3.6015e+00, -1.5942e+00,\n",
       "           3.3437e+00,  2.0924e+00, -3.5919e-01, -6.5799e-01, -1.2765e-01,\n",
       "           3.4359e-01,  1.8581e-01, -2.3663e+00,  4.5933e-01,  1.5553e+00,\n",
       "          -6.7268e-03,  5.0065e-01,  1.6660e+00, -1.1616e+00, -1.2553e-01,\n",
       "          -1.2817e+00,  1.5786e+00,  3.3612e+00, -2.1617e+00, -1.0733e+00,\n",
       "           2.1447e+00,  3.1276e+00, -4.1930e-01, -1.1110e+00, -2.4153e+00,\n",
       "          -2.7908e+00,  1.2843e+00,  1.8461e+00,  3.2900e-01,  1.1493e+00,\n",
       "          -2.3812e-01,  1.7211e+00, -2.5682e+00,  1.8069e+00,  1.8501e-01,\n",
       "          -3.2331e+00,  4.3830e+00, -2.2945e-01,  6.1584e-01,  2.4914e+00],\n",
       "         [ 3.9121e+00, -4.2628e+00, -1.5355e+00, -8.6138e-01, -5.3411e+00,\n",
       "           2.7108e+00, -4.8567e+00,  1.2639e+00,  4.1178e-01,  3.3814e-01,\n",
       "           1.1726e+00,  2.5120e+00, -4.5607e-01,  4.3441e+00, -1.2549e-01,\n",
       "           1.0772e+00,  5.3761e-01,  3.2290e-01,  1.1849e+00,  2.4327e+00,\n",
       "          -1.1294e+00,  7.9947e-01,  6.1777e-01, -2.5117e+00, -2.4288e+00,\n",
       "          -5.2846e-01,  4.2760e-01,  4.7329e-02, -1.7647e+00, -1.4752e+00,\n",
       "           1.1964e+00, -2.6248e+00,  5.7221e-01, -2.3122e+00, -2.6880e+00,\n",
       "          -4.7765e-01,  2.3314e-01,  2.7544e+00, -2.4019e+00,  2.4063e+00,\n",
       "          -3.6050e-01,  1.5268e+00, -1.0554e+00,  1.5301e-01, -4.0364e+00,\n",
       "          -7.5164e-01,  2.9147e+00,  1.2352e+00,  3.0499e+00,  1.4382e+00],\n",
       "         [-8.6187e-01, -3.3226e+00, -2.0926e+00,  3.2199e-01, -1.5216e+00,\n",
       "          -4.9067e-01, -1.2633e+00,  1.2386e+00, -2.8307e+00, -9.8313e-01,\n",
       "          -2.6500e+00, -1.1549e+00, -5.2447e-01, -1.0440e+00, -3.5639e+00,\n",
       "          -1.2792e+00,  3.7044e+00,  5.4222e-01, -1.1362e+00,  4.1150e+00,\n",
       "          -1.4463e+00, -1.5089e+00, -9.6748e-01, -8.6720e-01, -7.0662e-01,\n",
       "          -1.7037e+00, -1.5529e+00,  3.1837e+00,  2.7793e+00, -1.5951e+00,\n",
       "           1.6251e+00,  6.5077e-01, -1.7662e+00, -9.3005e-01,  1.8640e+00,\n",
       "           2.5678e+00,  1.7036e+00, -1.3591e+00,  1.3296e+00, -2.5949e+00,\n",
       "          -4.8426e-01,  4.1033e-01, -1.0311e+00,  3.5298e+00,  2.6853e+00,\n",
       "          -4.6994e+00,  1.6157e-01,  5.0802e+00,  2.1544e-01, -1.5236e+00],\n",
       "         [-2.8986e+00,  4.0422e+00,  2.1768e+00,  9.4471e-01,  5.0561e+00,\n",
       "          -1.1749e+00,  5.0173e+00, -7.4158e-01, -1.4869e+00, -9.1994e-01,\n",
       "           1.3684e+00, -1.2900e+00,  4.8484e-01, -4.8584e+00,  1.2330e+00,\n",
       "          -5.9745e-01, -1.7067e+00, -1.8186e+00, -7.0141e-01, -3.2112e+00,\n",
       "           1.7269e+00, -1.2790e-01,  5.2936e-01,  2.4534e+00,  2.8698e+00,\n",
       "           4.5033e-01,  8.8490e-01,  7.2982e-01, -1.1029e-01,  1.5832e+00,\n",
       "          -7.0171e-01,  4.4080e+00, -2.8902e-01,  2.2789e+00,  1.0801e+00,\n",
       "          -1.8168e+00, -1.1974e-01, -1.6271e+00,  2.4221e+00, -1.2136e+00,\n",
       "           4.3078e-01, -9.0337e-01,  1.2145e-01, -3.4287e-01,  3.7240e+00,\n",
       "           6.1895e-01, -9.2741e-01, -3.1579e+00, -3.1364e+00,  2.4332e-01]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = [0.6751, 0.1021, 1.4837, 0.1549]\n",
    "# mean = sum(x)/len(x)\n",
    "# std = (sum([(i-mean)**2 for i in x])/3)**0.5\n",
    "# [(i-mean)/(std+10**-5) for i in x]\n",
    "\n",
    "# sum(x)/len(x)\n",
    "# x - \n",
    "# x - x.mean(dim=-1, keepdim=True)\n",
    "# x.std(dim=-1) \n",
    "\n",
    "\n",
    "\n",
    "# for name, param in dropout.named_parameters():\n",
    "#     print(name, type(param), param.size())\n",
    "\n",
    "\n",
    "# dropout_out\n",
    "final_output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea72b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(0.0775)\n",
    "tensor(6.7979)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81fadd",
   "metadata": {},
   "source": [
    "# Testing how close the values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015e5d3",
   "metadata": {},
   "source": [
    "# Things to note\n",
    "1. Inverted Dropout vs Regular Dropout: \n",
    "https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch\n",
    "Regular Dropout\n",
    "During training, randomly set a fraction p of input units to zero. At test time, scale is not applied â€” raw values are passed as is. This causes a mismatch between train and test time behavior. Therefore in regular dropout, the test time input units are multiplied with (1-p) to scale them down. This results in modifying test time model based on the dropout chosen.\n",
    "\n",
    "Inverted Dropout\n",
    "During training, randomly drop units with probability p, but scale the remaining active units by 1 / (1 - p). At test time, no scaling is needed â€” output is already normalized during training.\n",
    "\n",
    "2. Why are biases removed in the attention layers? The attention layers in transformers are usually followed by normalization layers which effectively remove the bias. Addind bias thus is nothing but an overhead. https://ai.stackexchange.com/questions/40252/why-are-biases-typically-not-used-in-attention-mechanism\n",
    "\n",
    "3. Which attention scores are masked with padding?\n",
    "Padding tokens are added to equalize sequence lengths for efficient batchingâ€”not for learning. To avoid text tokens learning from pads, we mask the key positions of pad tokens during attention. This prevents queries (real tokens) from attending to them.\n",
    "But why not mask query positions of pad tokens?\n",
    "As explained in this blog, masking pad queries results in a uniform attention distribution (1/n), causing their context vectors to become just averages of other tokens. This erases any distinct signal and is usually unnecessary, since outputs from pad positions are typically ignored later.\n",
    "https://gmongaras.medium.com/how-do-self-attention-masks-work-72ed9382510f\n",
    "\n",
    "4. masked_fill_ does in place masking and broadcasts mask as long as it has the right dimensions\n",
    "\n",
    "5. unsqueeze adds a dimension 1 in the specified dim value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5e824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba219ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = torch.distributions.Bernoulli(0.7)\n",
    "# a  = m.sample((2, 2, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58cc5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(2, 3)\n",
    "# pos_embeddings = nn.Embedding(4, 3)\n",
    "# batch_size, seq_len = x.shape\n",
    "# pos_embeddings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6219e907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0588, -1.0991,    -inf],\n",
       "          [ 1.3585, -0.4391,    -inf],\n",
       "          [ 2.9100,  0.2492,    -inf]],\n",
       "\n",
       "         [[-0.1958, -0.9341,    -inf],\n",
       "          [-0.6880,  0.2391,    -inf],\n",
       "          [-0.4189, -1.3787,    -inf]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8609,    -inf,    -inf],\n",
       "          [ 0.8796,    -inf,    -inf],\n",
       "          [-2.5083,    -inf,    -inf]],\n",
       "\n",
       "         [[ 1.3594,    -inf,    -inf],\n",
       "          [-0.7312,    -inf,    -inf],\n",
       "          [-0.7218,    -inf,    -inf]]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "attention_scores = torch.randn(2, 2, 3, 3)\n",
    "attention_mask = torch.tensor([[1, 1, 0],[1, 0, 0]])\n",
    "# mask = \n",
    "# \n",
    "mask = torch.triu(torch.ones(2, 3, 3), diagonal=1)\n",
    "# attention_mask = 1-attention_mask\n",
    "# attention_mask = attention_mask.reshape(2, 1, -1)\n",
    "# mask = mask + attention_mask\n",
    "\n",
    "# print(attention_mask)\n",
    "attention_mask = (1-attention_mask).bool().unsqueeze(1).unsqueeze(2)\n",
    "print(attention_mask.shape)\n",
    "# attention_mask.expand(2, 2, 3, 3)\n",
    "attention_scores.masked_fill_(attention_mask, -np.inf)\n",
    "\n",
    "# torch.tril(temp)\n",
    "# temp - torch.max(temp, dim=-1, keepdim=True).values\n",
    "# temp.view(1, 2, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "090c5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "    \n",
    "# # Example 1: Simple chat interaction\n",
    "# response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n",
    "# print(response['message']['content'])\n",
    "\n",
    "# # Example 2: Streaming responses\n",
    "# stream = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': 'Tell me a story'}], stream=True)\n",
    "# for chunk in stream:\n",
    "#     print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "prompt = \"\"\"Can you optimize this code and rewrite it:\n",
    " def _merge_tokens(self, tokenized_text, max_freq_pair):\n",
    "        # This method merges the occurence of max_freq_pair tokens in tokenized_text\n",
    "        # tokenized_text - A dictionary of index to list of tokens mapping\n",
    "        # max_freq_pair - a tuple of two tokens which have the max frequency of occuring\n",
    "        for i, tokens in tokenized_text.items():\n",
    "            merge_indices = set()\n",
    "            j = 0\n",
    "            while j < (len(tokens)-1):\n",
    "                if tokens[j] == max_freq_pair[0] and tokens[j+1] == max_freq_pair[1]:\n",
    "                    merge_indices.add(j)\n",
    "                    j += 1\n",
    "                j += 1\n",
    "            updated_tokens = []\n",
    "            j = 0\n",
    "            while j< len(tokens):\n",
    "                if j in merge_indices:\n",
    "                    updated_tokens.append(tokens[j]+tokens[j+1])\n",
    "                    j += 1\n",
    "                else:\n",
    "                    updated_tokens.append(tokens[j])\n",
    "                j += 1\n",
    "            tokenized_text[i] = updated_tokens\n",
    "        return tokenized_text\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Is there any optimal way to write this code by combining both sample and reshape methods\n",
    "bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([1-self.prob]))\n",
    "return bernoulli_dist.sample(x.shape).reshape(x.shape)\n",
    "\"\"\"\n",
    "# Example 3: Generate text\n",
    "response = ollama.generate(model='qwen2.5-coder:latest', prompt=prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40859af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! You can combine the `sample` and `reshape` methods more efficiently by using a single operation to generate samples with the desired shape directly. Here's an optimized version of your code:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "def sample_bernoulli_like(x, prob):\n",
      "    bernoulli_dist = torch.distributions.Bernoulli(1 - prob)\n",
      "    return bernoulli_dist.sample(x.shape)\n",
      "\n",
      "# Example usage:\n",
      "x = torch.randn(3, 4)  # Replace with your actual tensor\n",
      "prob = 0.5  # Replace with your actual probability\n",
      "sampled_tensor = sample_bernoulli_like(x, prob)\n",
      "print(sampled_tensor)\n",
      "```\n",
      "\n",
      "In this optimized version:\n",
      "- We create a `Bernoulli` distribution directly using the desired probability.\n",
      "- We use the `.sample()` method on the distribution and pass `x.shape` as an argument to generate samples with the same shape as `x`.\n",
      "\n",
      "This approach avoids unnecessary reshaping operations, making the code more concise and potentially more efficient.\n"
     ]
    }
   ],
   "source": [
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d146ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d54737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf05c2bb",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bac083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 1, 0, 0]]])\n",
      "tensor([[[[1, 1, 1, 0, 0]]]])\n",
      "tensor([[[[1, 1, 1, 0, 0]]]])\n",
      "tensor([[[[0.7374, 0.0480, 0.0809,   -inf,   -inf],\n",
      "          [0.3284, 0.6390, 0.8858,   -inf,   -inf],\n",
      "          [0.4830, 0.2081, 0.7727,   -inf,   -inf],\n",
      "          [0.4163, 0.5497, 0.0868,   -inf,   -inf],\n",
      "          [0.4099, 0.7877, 0.0380,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# In data loaders, we use multiple workers to load the data in parallel so that GPU doesn't sit idle. Note: DataParallelism is different from using workers. While the former is GPU side optimization the latter is CPU side. Coming to the original point, inside each worker pytorch by default sets the random seed as parent's random seed + worker_id. However seeds for other libraries may be duplicated during worker initialization, causing workers to produce identical random numbers. therefore, we initialize the seed for other libraries and pass it in the worker_init_fn of the Dataloader. For more details check - https://pytorch.org/docs/stable/data.html#data-loading-randomness\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af1a31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
