{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gradio langchain chromadb\n",
    "# ollama pull llama3.2:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e62818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "from utils import timeit\n",
    "import chromadb\n",
    "import hashlib\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f70e40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentParser(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self._validate_path()\n",
    "    \n",
    "    def _validate_path(self):\n",
    "        is_valid = True\n",
    "        message = None\n",
    "        if not os.path.exists(self.file_path):\n",
    "            is_valid = False\n",
    "            message = f\"The provided path {self.file_path} doesn't exist. Please recheck and provide the correct path.\"\n",
    "            return is_valid, message\n",
    "        elif not (self.file_path.endswith('.txt') or self.file_path.endswith('.pdf')):\n",
    "            is_valid = False\n",
    "            message = f\"Only .txt and .pdf extensions are supported. Please recheck and provide the correct path.\"\n",
    "            return is_valid, message\n",
    "        file_size_bytes = os.path.getsize(self.file_path)\n",
    "        if file_size_bytes>1000000:\n",
    "            is_valid = False\n",
    "            return is_valid, f\"Large File. Cannot process. Current file size: {file_size_bytes}>1000000\"\n",
    "        return is_valid, message\n",
    "    \n",
    "    def load_document(self):\n",
    "        if self.file_path.endswith(\".txt\"):\n",
    "            loader = TextLoader(self.file_path)\n",
    "        elif self.file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(self.file_path)\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    \n",
    "    def split_document(self, documents):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "        text_chunks = text_splitter.split_documents(documents)\n",
    "        return text_chunks\n",
    "\n",
    "    @timeit\n",
    "    def parse(self):\n",
    "        documents = self.load_document()\n",
    "        text_chunks = self.split_document(documents)\n",
    "        return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8641a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ead326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(object):\n",
    "    def __init__(self, model_name='nomic-embed-text'):\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def embed(self, doc):\n",
    "        #returns the embedding of a single chunk\n",
    "        return ollama.embed(\n",
    "                model=self.model_name,\n",
    "                input=doc,\n",
    "            ).embeddings\n",
    "\n",
    "\n",
    "class VectorStore(object):\n",
    "    def __init__(self, db_path=\"./chroma_db\", collection_name=\"docrag\"):\n",
    "        # calling a persistent client to save and load database from local machine\n",
    "        self.database_client = chromadb.PersistentClient(path=db_path)\n",
    "        self.collection_name = collection_name\n",
    "        # creates a new collection, like a folder for each logically seperate vector stores\n",
    "        self.collection = self.database_client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    def add_chunk(self, chunk_id, chunk_text, chunk_embedding):\n",
    "        # add the chunk's embedding, text into the vector store and maps it to the provided id\n",
    "        self.collection.add(ids=chunk_id, embeddings=chunk_embedding, documents=chunk_text)\n",
    "    \n",
    "    def add_chunks(self, chunk_ids, chunk_texts, chunk_embeddings):\n",
    "        self.collection.add(ids=chunk_ids, embeddings=chunk_embeddings, documents=chunk_texts)\n",
    "    \n",
    "    def retrieve(self, query_embedding, top_k=5):\n",
    "        # inbuilt query function provided by chromaDB, retrives the top k similar documents from the DB. \n",
    "        # by default chrom\n",
    "        results = self.collection.query(query_embedding, n_results=top_k, include=[\"documents\"])\n",
    "        return results['documents'][0]\n",
    "\n",
    "class Indexer(object):\n",
    "    def __init__(self, vector_store, embedder):\n",
    "        self.embedder = embedder\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def create_index(self, docs):\n",
    "        for chunk in docs:\n",
    "            text = chunk.page_content\n",
    "            embedding = self.embedder.embed(text)[0]\n",
    "            chunk_id = str(hashlib.md5(text.encode()).hexdigest())\n",
    "            self.vector_store.add_chunk(chunk_id, text, embedding)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb88a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LLM(object):\n",
    "    def __init__(self, model_name='mistral'):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    @timeit\n",
    "    def call(self, user_message=None):\n",
    "        messages = None\n",
    "        if user_message: \n",
    "            messages = self._add_message('user', user_message)\n",
    "        response = ollama.chat(model=self.model_name, messages=messages)\n",
    "        return self._get_content_from_response(response) \n",
    "\n",
    "    def _get_content_from_response(self, response):\n",
    "        return response['message']['content']\n",
    "\n",
    "    def _add_message(self, role, message):\n",
    "        messages = [{'role': role, 'content': message}]\n",
    "        return messages\n",
    "\n",
    "class DocRAG(object):\n",
    "    def __init__(self, file_path, llm_model_name):\n",
    "        self.file_path = file_path\n",
    "        self.llm_model_name = llm_model_name\n",
    "        self.llm = LLM(model_name=self.llm_model_name)\n",
    "        self.document_parser = DocumentParser(file_path=self.file_path)\n",
    "        self.embedder = Embedding()\n",
    "        document_identifier = self._get_document_identifier()\n",
    "        print(\"Creating collection \", document_identifier)\n",
    "        self.collection_name = document_identifier\n",
    "        self.vector_store = VectorStore(collection_name=self.collection_name)\n",
    "        self.indexer = Indexer(vector_store=self.vector_store, embedder=self.embedder)\n",
    "        self.process_document()\n",
    "    \n",
    "    def _get_document_identifier(self):\n",
    "        document_identifier = self.file_path.split('/')[-1].replace(' ', '_').replace('.', '_')[:64]\n",
    "        return document_identifier\n",
    "\n",
    "    def process_document(self):\n",
    "        document_chunks = self.document_parser.parse()\n",
    "        self.indexer.create_index(document_chunks)\n",
    "    \n",
    "    def generate_query(self, user_question):\n",
    "        user_message = f\"\"\"You are a query rewriting assistant. Your task is to rewrite the user's question into a search query that will retrieve the most relevant documents from a knowledge base. The name of the file is {self.file_path.split(\"/\")[-1]}\n",
    "        Instructions:\n",
    "        - Keep the query concise.\n",
    "        - Use key concepts, keywords, and entities mentioned in the original question.\n",
    "        - Do not include phrases like \"Find documents about...\" or \"Search for...\".\n",
    "        - Just output the improved query.\n",
    "        - Generate only one query dont use OR and AND connecters\n",
    "\n",
    "        Original Question:\n",
    "        {user_question}\n",
    "\n",
    "        Rewritten Search Query:\"\"\"\n",
    "        output = self.llm.call(user_message=user_message)\n",
    "        return output\n",
    "\n",
    "    def get_answer(self, query, user_question):\n",
    "        context = self.query_db(query=query)\n",
    "        print(\"Following context retrieved from database\", context)\n",
    "        message = f\"\"\"You are a helpful assistant. Use the provided context to answer the user's question.\n",
    "\n",
    "        If the answer is not present in the context, say \"I don't have enough information based on the provided documents.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {user_question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        output = self.llm.call(user_message=message)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def query_db(self, query):\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        context = self.vector_store.retrieve(query_embedding=query_embedding)\n",
    "        return context\n",
    "\n",
    "    def chat(self, user_question):\n",
    "        query = self.generate_query(user_question=user_question)\n",
    "        print(f\"Querying database: {query}\")\n",
    "        answer = self.get_answer(query=query, user_question=user_question)\n",
    "        return answer\n",
    "    \n",
    "class DocRAGApp():\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.doc_rag_pipeline = None\n",
    "\n",
    "    def process_document(self, file_path):\n",
    "        self.doc_rag_pipeline = DocRAG(\n",
    "            file_path=file_path,\n",
    "            llm_model_name=self.model_name\n",
    "        )\n",
    "        return \"Documents successfully processed and indexed.\"\n",
    "    \n",
    "    def chat(self, user_question):\n",
    "        return self.doc_rag_pipeline.chat(user_question=user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b06490d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "doc_rag = DocRAGApp('llama3.2:latest')\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## DocRAG: Local Retrieval-Augmented Chatbot for your docs\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(file_types=[\".pdf\", \".txt\"], label=\"Upload Document\", file_count=\"single\")\n",
    "        upload_button = gr.Button(\"Process File\")\n",
    "    \n",
    "    upload_output = gr.Textbox(label=\"Upload Status\")\n",
    "\n",
    "    question_input = gr.Textbox(label=\"Ask a Question\")\n",
    "    ask_button = gr.Button(\"Get Answer\")\n",
    "    answer_output = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    # Upload handler\n",
    "    upload_button.click(fn=doc_rag.process_document, inputs=[file_input], outputs=[upload_output])\n",
    "\n",
    "    # Ask handler\n",
    "    ask_button.click(fn=doc_rag.chat, inputs=[question_input], outputs=[answer_output])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133c5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
